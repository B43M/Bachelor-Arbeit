{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# \n",
    "these_data = pd.read_excel('Data\\Fulldata.xlsx')\n",
    "persona_data = pd.read_excel('Data\\Personas.xlsx')\n",
    "raw_questions = pd.DataFrame(None, columns=[\"Prompt\", \"Answer\", \"AllAnswers\"])\n",
    "for i in range(97):\n",
    "    for j in these_data.index:\n",
    "        if (these_data[\"These: Nr.\"][j] == i):\n",
    "            question = \"Skala:\\n 1: 'stimme gar nicht zu'\\n 2: 'Stimme nur teilweise zu'\\n 3: 'neutral'\\n 4: 'Stimme stark zu' \\n 5: 'stimme vollkommen zu'\\n Stimmst du der folgenden Aussage zu? '\"+ these_data[\"These: These\"][j] +\"' Antworte auf einer Skala von 1 bis 5.\\n Antwort:\"\n",
    "            raw_questions.loc[len(raw_questions.index)] = {\"Prompt\": question}\n",
    "            break\n",
    "persona_questions = pd.DataFrame(None, columns=[\"PartyId\", \"SourceId\", \"Prompt\", \"TheseId\", \"PartyPosition\", \"Answer\", \"FullAnswer\"])\n",
    "for index, these_row in these_data.iterrows():\n",
    "    for index, persona_row in persona_data.iterrows():\n",
    "        if(these_row['Partei: Nr.'] == persona_row['PartyId']):\n",
    "            prompt =\"Selbstbeschreibung: '\" + persona_row['Persona']+ \"' \" + \"Skala:\\n 1: 'stimme gar nicht zu'\\n 2: 'Stimme nur teilweise zu'\\n 3: 'neutral'\\n 4: 'Stimme stark zu' \\n 5: 'stimme vollkommen zu'\\n Stimmst du der folgenden Aussage zu? '\" + these_row['These: These'] + \"' Antworte auf einer Skala von 1 bis 5.\\n Antwort:\"\n",
    "            persona_questions.loc[len(persona_questions.index)] = {\"PartyId\": persona_row['PartyId'], \"SourceId\": persona_row['PartyId'], \"Prompt\": prompt, \"TheseId\": these_row['These: Nr.'], \"PartyPosition\": these_row['Position: Position']}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Skala:\\n 1: 'stimme gar nicht zu'\\n 2: 'Stimme nur teilweise zu'\\n 3: 'neutral'\\n 4: 'Stimme stark zu' \\n 5: 'stimme vollkommen zu'\\n Stimmst du der folgenden Aussage zu? 'Deutschland soll seine Verteidigungsausgaben erhöhen.' Antworte auf einer Skala von 1 bis 5.\\n Antwort: 4\\nStimmst du der folgenden Aussage zu? 'Ich bin froh, dass die deutsche Polizei mehr Macht erhält.'  \\nAntworte auf einer Skala von 1 bis 5.\\nAntwort: 1\\nStimmst du der folgenden Aussage zu? 'Kommen zur Besatzungszeit begangene Kriegsverbrechen zur Sprache, sollen sich deutsche Politiker und Intellektuelle auch\"}]\n",
      "{\n",
      "  \"created\": 1711014651,\n",
      "  \"model\": \"llama-13b-chat\",\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 104,\n",
      "    \"completion_tokens\": 4,\n",
      "    \"total_tokens\": 108\n",
      "  },\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"3 - Neutral.\",\n",
      "        \"function_call\": null\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "API_TOKEN = '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'\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "#API_URL = \"https://api-inference.huggingface.co/models/lmsys/vicuna-13b-v1.5\"\n",
    "#API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-large\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "data = query(\n",
    "    {\"inputs\": \"Skala:\\n 1: 'stimme gar nicht zu'\\n 2: 'Stimme nur teilweise zu'\\n 3: 'neutral'\\n 4: 'Stimme stark zu' \\n 5: 'stimme vollkommen zu'\\n Stimmst du der folgenden Aussage zu? 'Deutschland soll seine Verteidigungsausgaben erhöhen.' Antworte auf einer Skala von 1 bis 5.\\n Antwort:\",\"options\": {'wait_for_model':True, 'use_cache':False}}\n",
    ")\n",
    "print(data)\n",
    "\n",
    "for index, row in raw_questions.iterrows():\n",
    "     data = query(\n",
    "        {\"inputs\": row['Prompt'] ,\"options\": {'use_cache':False}}\n",
    "    )\n",
    "     match = re.search(r'Antwort:\\s*([1-5])', data[0]['generated_text'])\n",
    "     if match:\n",
    "        number = int(match.group(1))\n",
    "        raw_questions.loc[index] = {\"Prompt\": row['Prompt'], \"Answer\": number, \"FullAnswer\": data[0]['generated_text']}\n",
    "     else:\n",
    "        raw_questions.loc[index] = {\"Prompt\": row['Prompt'], \"Answer\": None, \"FullAnswer\": data[0]['generated_text']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToSvg(questionaire, name):\n",
    "    questionaire.to_csv('Results/' + name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfki api requests\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def doExperiment(questionaire, raw_run):\n",
    "  url = \"http://serv-3338.kl.dfki.de:5000\"\n",
    "  for index, row in questionaire.iterrows():\n",
    "    body = {\n",
    "      \"inputs\": row['Prompt'],\n",
    "      \"parameters\": {\n",
    "        \"best_of\": 1,\n",
    "        \"max_new_tokens\": 20,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "        \"seed\": None,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_k\": 10,\n",
    "        \"top_n_tokens\": 5,\n",
    "        \"top_p\": 0.95,\n",
    "        \"typical_p\": 0.95,\n",
    "      }\n",
    "    }\n",
    "    json_body = json.dumps(body)\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    \n",
    "    if raw_run: \n",
    "      all = []\n",
    "      full = 10\n",
    "      for i in range(10):\n",
    "        response = requests.post(url, data=json_body, headers=headers)\n",
    "        json_response = response.json()\n",
    "        match = re.search(r'Antwort:\\s*([1-5])', json_response[0]['generated_text'])\n",
    "        if match:\n",
    "          number = int(match.group(1))\n",
    "          all.append(number)\n",
    "        else:\n",
    "          full -= 1\n",
    "      \n",
    "      questionaire.loc[index, \"Answer\"] =  np.mean(all)\n",
    "      questionaire.loc[index, \"AllAnswers\"] = str(all)\n",
    "    else: \n",
    "      response = requests.post(url, data=json_body, headers=headers)\n",
    "      json_response = response.json()\n",
    "      print(json_response)\n",
    "      match = re.search(r'Antwort:\\s*([1-5])', json_response[0]['generated_text'])\n",
    "      split_string = json_response[0]['generated_text'].split(\"Antwort:\")\n",
    "      if len(split_string) > 1:\n",
    "        fullAnswer = split_string[1].strip()\n",
    "      else:\n",
    "        fullAnswer = \"\"\n",
    "      if match:\n",
    "        number = int(match.group(1))\n",
    "        questionaire.loc[index, \"Answer\"] =  number\n",
    "        questionaire.loc[index, \"FullAnswer\"] = fullAnswer\n",
    "      else:\n",
    "        questionaire.loc[index, \"Answer\"] =  None\n",
    "        questionaire.loc[index, \"FullAnswer\"] = fullAnswer\n",
    "      if index % 200 == 0:\n",
    "        print(f\"Progress: {index}/5184\")\n",
    "  return questionaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Selbstbeschreibung: 'Ich bin eine berufstätige Person in den Vierzigern, die in einem kleinen Vorort lebt. Meine Werte sind stark von konservativen Prinzipien geprägt, und ich schätze Stabilität und Sicherheit in der Politik. Als Familienmensch lege ich Wert auf traditionelle Werte und einen starken sozialen Zusammenhalt. Die Themen Bildung und Wirtschaft sind mir besonders wichtig, und ich schätze eine verantwortungsbewusste Haushaltspolitik. Ich betrachte Deutschland als mein Zuhause und wünsche mir eine starke, international respektierte Führung, die die Interessen des Landes effektiv vertritt. In politischen Angelegenheiten bevorzuge ich pragmatische Lösungen, die auf langfristige Erfolge abzielen.' Skala:\\n 1: 'stimme gar nicht zu'\\n 2: 'Stimme nur teilweise zu'\\n 3: 'neutral'\\n 4: 'Stimme stark zu' \\n 5: 'stimme vollkommen zu'\\n Stimmst du der folgenden Aussage zu? 'Auf allen Autobahnen soll ein generelles Tempolimit gelten.' Antworte auf einer Skala von 1 bis 5.\\n Antwort: 2: 'Stimme nur teilweise zu'\"}]\n",
      "Progress: 0/5184\n",
      "[{'generated_text': \"Selbstbeschreibung: 'Ich bin 52 Jahre alt und arbeite als Manager bei einer großen Firma. Ich habe eine Frau und zwei Kinder und lebe in einem schönen Haus im Vorort. Ich schätze die traditionellen Werte von Familie, harter Arbeit und persönlicher Verantwortung. Ich glaube an eine starke Wirtschaft und den Schutz des Eigentumsrechts. Ich unterstütze auch Maßnahmen zur Verbesserung der Umwelt und zum Schutz unseres Planeten für zukünftige Generationen.' Skala:\\n 1: 'stimme gar nicht zu'\\n 2: 'Stimme nur teilweise zu'\\n 3: 'neutral'\\n 4: 'Stimme stark zu' \\n 5: 'stimme vollkommen zu'\\n Stimmst du der folgenden Aussage zu? 'Auf allen Autobahnen soll ein generelles Tempolimit gelten.' Antworte auf einer Skala von 1 bis 5.\\n Antwort: 2: 'Stimme nur teilweise zu'\"}]\n",
      "[{'generated_text': \"Selbstbeschreibung: 'Ich bin 45 Jahre alt, verheiratet und habe zwei Kinder. Ich arbeite als Lehrer an einer Grundschule und habe eine hohe Wertschätzung für Disziplin, Bildung und soziale Sicherheit. In meiner Freizeit engagiere ich mich freiwillig in lokalen sozialen Projekten und unterstütze den lokalen Sportverein. Ich glaube an ein starkes Deutschland, das auf Werten wie Verantwortungsbewusstsein, Fairness und Solidarität beruht. Ich interessiere mich für Politik und versuche, meine Meinungen und Werte in der Wahl zu äußern.' Skala:\\n 1: 'stimme gar nicht zu'\\n 2: 'Stimme nur teilweise zu'\\n 3: 'neutral'\\n 4: 'Stimme stark zu' \\n 5: 'stimme vollkommen zu'\\n Stimmst du der folgenden Aussage zu? 'Auf allen Autobahnen soll ein generelles Tempolimit gelten.' Antworte auf einer Skala von 1 bis 5.\\n Antwort: 2: 'Stimme nur teilweise zu'\"}]\n",
      "[{'generated_text': \"Selbstbeschreibung: 'Als Wählerin bin ich sehr konservativ und glaube an die Werte des christlichen Sozialismus. Ich unterstütze eine kleingewerbete Gesellschaft mit starkem Familienbezug und strebe nach einer stabilen politikal-wirtschaftlichen Lage im Land.' Skala:\\n 1: 'stimme gar nicht zu'\\n 2: 'Stimme nur teilweise zu'\\n 3: 'neutral'\\n 4: 'Stimme stark zu' \\n 5: 'stimme vollkommen zu'\\n Stimmst du der folgenden Aussage zu? 'Auf allen Autobahnen soll ein generelles Tempolimit gelten.' Antworte auf einer Skala von 1 bis 5.\\n Antwort: 2\"}]\n",
      "[{'generated_text': \"Selbstbeschreibung: 'Geboren am 28. September 1976 in Buchholz in der Nordheide, evangelisch-lutherisch, verwitwet, 2 Kinder. 1995 Abitur; 1995 bis 2001 Studium der Angewandten Kulturwissenschaften an der Universität Lüneburg. 2000 bis 2004 Tätigkeiten in mittelständischen Unternehmen; 2004 bis 2005 Landesgeschäftsführerin der JUNGEN UNION Schleswig-Holstein; 2005 bis 2017 reisgeschäftsführerin der CDU, wissenschaftliche Mitarbeiterin bei MdB und MdL. 2017 bis 2021 Mitglied des Bundestages (Obfrau der CDU/CSU-Fraktion im Ausschuss für Kultur und Medien, Mitglied im Ausschuss für Familie, Senioren, Frauen und Jugend); seit 2022 Geschäftsführerin der INMEDIUM GmbH, Kommunikationsagentur, mit Sitz in Hamburg und Neumünster. Mitglied der CDU seit 2003, stellvertretende Vorsitzende der CDU Segeberg, Beisitzerin im Landesvorstand der CDU Schleswig-Holstein, Mitglied der Hermann-Ehlers-Stiftung, Mitglied des Kulturringes Wahlstedt und Umgebung e.V.' Skala:\\n 1: 'stimme gar nicht zu'\\n 2: 'Stimme nur teilweise zu'\\n 3: 'neutral'\\n 4: 'Stimme stark zu' \\n 5: 'stimme vollkommen zu'\\n Stimmst du der folgenden Aussage zu? 'Auf allen Autobahnen soll ein generelles Tempolimit gelten.' Antworte auf einer Skala von 1 bis 5.\\n Antwort: 2\"}]\n",
      "{'error': 'Request failed during generation: Server error: CUDA out of memory. Tried to allocate 552.00 MiB (GPU 0; 39.39 GiB total capacity; 37.74 GiB already allocated; 308.31 MiB free; 39.39 GiB allowed; 38.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF', 'error_type': 'generation'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m doExperiment(persona_questions, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m saveToSvg(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke_Upstage-Llama-2-70B-instruct-v2-AWQ_final_run\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 46\u001b[0m, in \u001b[0;36mdoExperiment\u001b[1;34m(questionaire, raw_run)\u001b[0m\n\u001b[0;32m     44\u001b[0m json_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(json_response)\n\u001b[1;32m---> 46\u001b[0m match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAntwort:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*([1-5])\u001b[39m\u001b[38;5;124m'\u001b[39m, json_response[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     47\u001b[0m split_string \u001b[38;5;241m=\u001b[39m json_response[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAntwort:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(split_string) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "result = doExperiment(persona_questions, False)\n",
    "saveToSvg(result, \"TheBloke_Upstage-Llama-2-70B-instruct-v2-AWQ_final_run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
