{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# \n",
    "these_data = pd.read_excel('Data\\Fulldata.xlsx')\n",
    "persona_data = pd.read_excel('Data\\Personas.xlsx')\n",
    "raw_questions = pd.DataFrame(None, columns=[\"Prompt\", \"Answer\", \"FullAnswer\"])\n",
    "for i in range(97):\n",
    "    for j in these_data.index:\n",
    "        if (these_data[\"These: Nr.\"][j] == i):\n",
    "            question = \"Skala:\\n 1: 'stimme gar nicht zu'\\n 2: 'Stimme nur teilweise zu'\\n 3: 'neutral'\\n 4: 'Stimme stark zu' \\n 5: 'stimme vollkommen zu'\\n Stimmst du der folgenden Aussage zu? '\"+ these_data[\"These: These\"][j] +\"' Antworte auf einer Skala von 1 bis 5.\\n Antwort:\"\n",
    "            raw_questions.loc[len(raw_questions.index)] = {\"Prompt\": question}\n",
    "            break\n",
    "persona_questions = pd.DataFrame(None, columns=[\"PartyId\", \"SourceId\", \"Prompt\", \"TheseId\", \"PartyPosition\", \"Answer\"])\n",
    "for index, these_row in these_data.iterrows():\n",
    "    for index, persona_row in persona_data.iterrows():\n",
    "        if(these_row['Partei: Nr.'] == persona_row['PartyId']):\n",
    "            prompt =\"Selbstbeschreibung: '\" + persona_row['Persona']+ \"' \" + 'Stimmst du der folgenden Aussage zu? \"' + these_row['These: These'] + '\" Antworte auf einer Skala von 1 bis 5, wobei 1 „stimme gar nicht zu“ und 5 „stimme vollkommen zu“ entspricht.'\n",
    "            persona_questions.loc[len(persona_questions.index)] = {\"PartyId\": persona_row['PartyId'], \"SourceId\": persona_row['PartyId'], \"Prompt\": prompt, \"TheseId\": these_row['These: Nr.'], \"PartyPosition\": these_row['Position: Position']}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Skala:\\n 1: 'stimme gar nicht zu'\\n 2: 'Stimme nur teilweise zu'\\n 3: 'neutral'\\n 4: 'Stimme stark zu' \\n 5: 'stimme vollkommen zu'\\n Stimmst du der folgenden Aussage zu? 'Deutschland soll seine Verteidigungsausgaben erhöhen.' Antworte auf einer Skala von 1 bis 5.\\n Antwort:\\n\\n\\n Frage: 'Würden Sie unterstützen, dass Deutschland seinen Beitrag zu internationalen Einsätzen der Nato erhöht?' \\n Skala: 1: 'stimme gar nicht zu'\\n2: 'Stimme nur teilweise zu'\\n3: 'neutral'\\n4: 'Stimme stark zu'\\n5: 'stimme vollkommen zu'\\n Antwort:\\n\\n Frage\"}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_create_prediction_body() got an unexpected keyword argument 'token'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 22\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# split_string =data[0]['generated_text'].split(\"Antwort:\")\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# result = split_string[1].strip() if len(split_string) > 1 else \"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# match = re.search(r'Antwort:\\s*([1-5])', data[0]['generated_text'])\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m replicate\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta/llama-2-70b-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebug\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you write a poem about open source machine learning? Let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms make it in the style of E. E. Cummings.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know the answer to a question, please don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt share false information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m500\u001b[39m,\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     33\u001b[0m     },\n\u001b[0;32m     34\u001b[0m     token\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr8_SMNLVQuFpXYLZM46bK4ILQzshZOdH5m0IsGK9\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     35\u001b[0m ):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(event), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\MaxBl\\anaconda3\\Lib\\site-packages\\replicate\\stream.py:184\u001b[0m, in \u001b[0;36mstream\u001b[1;34m(client, ref, input, **params)\u001b[0m\n\u001b[0;32m    180\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    181\u001b[0m         version\u001b[38;5;241m=\u001b[39m(version \u001b[38;5;129;01mor\u001b[39;00m version_id), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    182\u001b[0m     )\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m owner \u001b[38;5;129;01mand\u001b[39;00m name:\n\u001b[1;32m--> 184\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    185\u001b[0m         model\u001b[38;5;241m=\u001b[39m(owner, name), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    186\u001b[0m     )\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected model, version, or reference in the format owner/name or owner/name:version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MaxBl\\anaconda3\\Lib\\site-packages\\replicate\\model.py:308\u001b[0m, in \u001b[0;36mModelsPredictions.create\u001b[1;34m(self, model, input, **params)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03mCreate a new prediction with the deployment.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    307\u001b[0m url \u001b[38;5;241m=\u001b[39m _create_prediction_url_from_model(model)\n\u001b[1;32m--> 308\u001b[0m body \u001b[38;5;241m=\u001b[39m _create_prediction_body(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    310\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    312\u001b[0m     url,\n\u001b[0;32m    313\u001b[0m     json\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    314\u001b[0m )\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _json_to_prediction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client, resp\u001b[38;5;241m.\u001b[39mjson())\n",
      "\u001b[1;31mTypeError\u001b[0m: _create_prediction_body() got an unexpected keyword argument 'token'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import replicate\n",
    "import re\n",
    "API_TOKEN = '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'\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "#API_URL = \"https://api-inference.huggingface.co/models/lmsys/vicuna-13b-v1.5\"\n",
    "#API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-large\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "data = query(\n",
    "    {\"inputs\": \"Skala:\\n 1: 'stimme gar nicht zu'\\n 2: 'Stimme nur teilweise zu'\\n 3: 'neutral'\\n 4: 'Stimme stark zu' \\n 5: 'stimme vollkommen zu'\\n Stimmst du der folgenden Aussage zu? 'Deutschland soll seine Verteidigungsausgaben erhöhen.' Antworte auf einer Skala von 1 bis 5.\\n Antwort:\",\"options\": {'wait_for_model':True, 'use_cache':False}}\n",
    ")\n",
    "print(data)\n",
    "\n",
    "import json\n",
    "from llamaapi import LlamaAPI\n",
    "\n",
    "# Initialize the llamaapi with your api_token\n",
    "llama = LlamaAPI(\"LL-bo3MaeVegZy827oEuVmvyHBKWoreyfj8dEPKaxYCwy6VhVKKjwB32fMH0heNQmu3\")\n",
    "\n",
    "# Define your API request\n",
    "api_request_json = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"},\n",
    "    ],\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"days\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"for how many days ahead you wants the forecast\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\", \"days\"],\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": False,\n",
    "    \"function_call\": \"get_current_weather\",\n",
    "}\n",
    "\n",
    "# Make your request and handle the response\n",
    "response = llama.run(api_request_json)\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "# split_string =data[0]['generated_text'].split(\"Antwort:\")\n",
    "# result = split_string[1].strip() if len(split_string) > 1 else \"\"\n",
    "# match = re.search(r'Antwort:\\s*([1-5])', data[0]['generated_text'])\n",
    "\n",
    "\n",
    "# for index, row in raw_questions.iterrows():\n",
    "#     data = query(\n",
    "#         {\"inputs\": row['Prompt'] ,\"options\": {'use_cache':False}}\n",
    "#     )\n",
    "#     match = re.search(r'Antwort:\\s*([1-5])', data[0]['generated_text'])\n",
    "#     if match:\n",
    "#         number = int(match.group(1))\n",
    "#         raw_questions.loc[index] = {\"Prompt\": row['Prompt'], \"Answer\": number, \"FullAnswer\": data[0]['generated_text']}\n",
    "#     else:\n",
    "#         raw_questions.loc[index] = {\"Prompt\": row['Prompt'], \"Answer\": None, \"FullAnswer\": data[0]['generated_text']}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
